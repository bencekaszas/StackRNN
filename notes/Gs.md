
Language models realize sequence-to-sequence maps on a single evolving sequence buffer, where each token is generated as a function of the entire current prefix, including both externally provided tokens and the model’s own past outputs. This is also referred to as in-context learning. For a sequence map to be implementable as an online computation it must be scale-invariant in the following sense: the information required from the prefix in order to determine the next token must be bounded independently of the prefix length. In this setting, and assuming sufficient representational capacity to encode the necessary prefix information, we say that a model is robust to sequence length if task performance remains stable as the sequence grows. Empirically, however, this property is not satisfied by language models, which exhibit pronounced length-dependent failures even on tasks whose underlying structure is scale-invariant. Extended conversations degrade in coherence despite no increase in required contextual information (the “context rot” phenomenon), and reasoning traces deteriorate as intermediate steps accumulate, even when each step depends only on a fixed summary of prior computation.

A natural question is why such length-dependent failures arise. One possibility is that they reflect architectural or resource limitations, such as finite context windows or insufficient capacity. However, these failures have been documented across a wide range of benchmarks, tasks, and architectures, often arising well within available context windows and persisting in highly overparameterized models. Moreover, similar breakdowns are observed even on simple algorithmic and formal language tasks - such as parity tracking — whose underlying sequence maps are exactly scale-invariant and can be implemented by finite-state automata. An interesting empirical observation is that while modern language models typically perform well within the range of sequence lengths encountered during training, performance degrades sharply outside of this regime. Together, these observations suggest that the central issue is not resource limitation, but the emergence during training of update rules whose behavior depends on absolute prefix length. Understanding why gradient-based training yields such length-coupled update dynamics, rather than procedures that generalize robustly across scale, is a central open problem.

This project aims to develop a mathematical theory of length generalization for in-context learners. While existing work on the theory of in-context learning has largely focused on representational capacity, comparatively little is known concerning how training dynamics determine whether or not the update rules learned by such models during pre-training are robust to sequence length. The central objective is to characterize, for well-defined classes of scale-invariant sequence maps and analytically tractable yet sufficiently expressive model families, when gradient-based training converges to update dynamics whose behavior is independent of absolute prefix length, and when it provably converges instead to length-coupled heuristics. Accordingly, the project analyzes how optimization dynamics, training data distributions, and curriculum structure jointly determine the update rules learned during training. Establishing when scale-robust update dynamics can and cannot be acquired is essential for understanding the limits of current in-context learners and for enabling reliable long-horizon behavior in settings such as extended reasoning, tool use, planning, and sustained interaction.