{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25d22d16",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c7e0f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "import optax\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "SEQ_LENGTH = 60       \n",
    "HIDDEN_DIM = 64\n",
    "STACK_DEPTH = SEQ_LENGTH\n",
    "LEARNING_RATE = 1e-3\n",
    "STEPS = 2000\n",
    "\n",
    "# input\n",
    "VOCAB_PAD, VOCAB_0, VOCAB_1, VOCAB_EQ = 0, 1, 2, 3\n",
    "VOCAB_SIZE = 4\n",
    "\n",
    "# stack\n",
    "STACK_NULL, STACK_0, STACK_1 = 0, 1, 2\n",
    "STACK_VOCAB_SIZE = 3\n",
    "\n",
    "# memory actions\n",
    "ACT_NOOP, ACT_PUSH_0, ACT_PUSH_1, ACT_POP = 0, 1, 2, 3\n",
    "NUM_MEM_ACTIONS = 4\n",
    "\n",
    "# buffer\n",
    "OUT_NOOP, OUT_EMIT_0, OUT_EMIT_1 = 0, 1, 2\n",
    "NUM_BUF_ACTIONS = 3\n",
    "\n",
    "#controller states\n",
    "STATE_READ = 0\n",
    "STATE_WRITE = 1\n",
    "NUM_STATES = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45b6d45",
   "metadata": {},
   "source": [
    "### Check whether using GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7c6c12c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAX Backend: gpu\n",
      "Devices: [CudaDevice(id=0)]\n"
     ]
    }
   ],
   "source": [
    "print(\"JAX Backend:\", jax.default_backend())\n",
    "print(\"Devices:\", jax.devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0ae9b9",
   "metadata": {},
   "source": [
    "--- \n",
    "# Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f396622b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " [[2 1 1 2 2 1 1 1 2 2 1 2 2 1 2 2 2 2 2 1 2 1 2 3 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "Target Mem Actions:\n",
      " [[2 1 1 2 2 1 1 1 2 2 1 2 2 1 2 2 2 2 2 1 2 1 2 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      "  3 3 3 3 3 3 3 3 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 2 2 3 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "Target Buffer Actions:\n",
      " [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2 1 2 2 2 2 2 1 2 2\n",
      "  1 2 2 1 1 1 2 2 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "Target States:\n",
      " [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "def generate_rev_trace(batch_size, seq_length=SEQ_LENGTH):\n",
    "    \"\"\"\n",
    "    Generate data for the reverse task with full traces\n",
    "    \"\"\"\n",
    "\n",
    "    lengths = np.random.randint(1, seq_length + 1, size=batch_size)\n",
    "    seq_len = 2 * seq_length + 1  # input + = + output \n",
    "\n",
    "    inputs = np.full((batch_size, seq_len), VOCAB_PAD, dtype=np.int32)\n",
    "    target_act = np.full((batch_size, seq_len), ACT_NOOP, dtype=np.int32)\n",
    "    target_buf = np.full((batch_size, seq_len), OUT_NOOP, dtype=np.int32)\n",
    "    target_state = np.full((batch_size, seq_len), STATE_READ, dtype=np.int32)\n",
    "    \n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        L = lengths[i]\n",
    "        bits = np.random.randint(1, 3, size=L)\n",
    "        \n",
    "        # read \n",
    "        # Input: bits + '='\n",
    "        inputs[i, :L] = bits\n",
    "        inputs[i, L] = VOCAB_EQ\n",
    "        \n",
    "        pop_start = L # start at equal!\n",
    "        pop_end = pop_start + L\n",
    "        \n",
    "        target_act[i, :L] = bits\n",
    "        \n",
    "        # State\n",
    "        target_state[i, :L] = STATE_READ\n",
    "        target_state[i, L] = STATE_WRITE \n",
    "        \n",
    "        # write\n",
    "        target_act[i,pop_start : pop_end] = ACT_POP\n",
    "        \n",
    "        # buffer: Emit the popped bits \n",
    "        # DELAYED BY 1!!\n",
    "        reversed_bits = bits[::-1]\n",
    "        target_buf[i, pop_start+1 : pop_end+1] = reversed_bits \n",
    "        \n",
    "        # State: stays in WRITE\n",
    "        target_state[i, pop_start : pop_end] = STATE_WRITE\n",
    "\n",
    "            \n",
    "    return jnp.array(inputs), jnp.array(target_act), jnp.array(target_buf), jnp.array(target_state)\n",
    "\n",
    "\n",
    "# test\n",
    "inputs, tgt_a, tgt_b, tgt_s = generate_rev_trace(2)\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"Target Mem Actions:\\n\", tgt_a)\n",
    "print(\"Target Buffer Actions:\\n\", tgt_b)\n",
    "print(\"Target States:\\n\", tgt_s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5e0fd0",
   "metadata": {},
   "source": [
    "---\n",
    "# Stack Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5e095ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: 1, Stack: [1 0 0 0 0], Ptr: 1, R_t: 0\n",
      "Action: 2, Stack: [1 2 0 0 0], Ptr: 2, R_t: 0\n",
      "Action: 3, Stack: [1 0 0 0 0], Ptr: 1, R_t: 2\n",
      "Action: 3, Stack: [0 0 0 0 0], Ptr: 0, R_t: 1\n"
     ]
    }
   ],
   "source": [
    "def update_stack(stack, ptr, action):\n",
    "    \"\"\"\n",
    "    stack update\n",
    "    \n",
    "    stack: [Depth]\n",
    "    ptr: Scalar index\n",
    "    action: Scalar action\n",
    "    \"\"\"\n",
    "    # Actions:\n",
    "    # NOOP: No change.\n",
    "    # PUSH 0: stack[ptr] = STACK_0, ptr++\n",
    "    # PUSH 1: stack[ptr] = STACK_1, ptr++\n",
    "    # POP: val = stack[ptr-1], ptr--\n",
    "    \n",
    "    # READ\n",
    "    pop_ptr = jnp.maximum(0, ptr - 1)\n",
    "    popped_val = stack[pop_ptr]\n",
    "    \n",
    "    # WRITE\n",
    "    push_val = action \n",
    "    \n",
    "    # UPDATE\n",
    "    is_push = (action == ACT_PUSH_0) | (action == ACT_PUSH_1)\n",
    "    new_stack_push = stack.at[ptr].set(push_val)\n",
    "    new_ptr_push = ptr + 1\n",
    "    \n",
    "    is_pop = (action == ACT_POP)\n",
    "    new_ptr_pop = pop_ptr\n",
    "\n",
    "    new_stack_pop = stack.at[pop_ptr].set(STACK_NULL) \n",
    "    \n",
    "    # Combine\n",
    "    stack = jnp.where(is_push, new_stack_push, stack)\n",
    "    stack = jnp.where(is_pop, new_stack_pop, stack)\n",
    "    \n",
    "    ptr = jnp.where(is_push, new_ptr_push, ptr)\n",
    "    ptr = jnp.where(is_pop, new_ptr_pop, ptr)\n",
    "    \n",
    "    r_t = jnp.where(is_pop, popped_val, STACK_NULL)\n",
    "    \n",
    "    return stack, ptr, r_t\n",
    "\n",
    "#test \n",
    "stack = jnp.array([0,0,0,0,0])\n",
    "ptr = 0\n",
    "actions = [ACT_PUSH_0, ACT_PUSH_1, ACT_POP, ACT_POP]\n",
    "for a in actions:\n",
    "    stack, ptr, r = update_stack(stack, ptr, a)\n",
    "    print(f\"Action: {a}, Stack: {stack}, Ptr: {ptr}, R_t: {r}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91e14e7",
   "metadata": {},
   "source": [
    "--- \n",
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac7d9e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackMachineCell(nn.Module):\n",
    "    \"\"\"\n",
    "    Single step of the machine.\n",
    "    Wrapped in nn.scan to handle recurrence.\n",
    "    \"\"\"\n",
    "    stack_depth: int = STACK_DEPTH\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, carry, inputs):\n",
    "        \n",
    "        #unpack carry and inp\n",
    "        stack, ptr, r_prev, s_prev = carry\n",
    "        x_t, true_act, true_s, use_forcing = inputs\n",
    "        \n",
    "        #controller\n",
    "        x_emb = nn.Embed(VOCAB_SIZE, HIDDEN_DIM)(x_t)\n",
    "        s_emb = jax.nn.one_hot(s_prev, NUM_STATES)\n",
    "        r_emb = jax.nn.one_hot(r_prev, STACK_VOCAB_SIZE)\n",
    "        \n",
    "        flat_input = jnp.concatenate([x_emb, s_emb, r_emb], axis=-1)\n",
    "        \n",
    "        logits_mem = nn.Dense(NUM_MEM_ACTIONS, name=\"head_mem\")(flat_input)\n",
    "        logits_buf = nn.Dense(NUM_BUF_ACTIONS, name=\"head_buf\")(flat_input)\n",
    "        logits_state = nn.Dense(NUM_STATES, name=\"head_state\")(flat_input)\n",
    "        \n",
    "        # decide actions\n",
    "        pred_act = jnp.argmax(logits_mem, axis=-1)\n",
    "        pred_state = jnp.argmax(logits_state, axis=-1)\n",
    "        \n",
    "        # If use_forcing is True, use true_act,else use pred_act\n",
    "        action_to_exec = jnp.where(use_forcing > 0, true_act, pred_act)\n",
    "        next_s = jnp.where(use_forcing > 0, true_s, pred_state)\n",
    "        \n",
    "        # Update Stack\n",
    "        stack_new, ptr_new, r_new = jax.vmap(update_stack)(stack, ptr, action_to_exec)\n",
    "        \n",
    "        new_carry = (stack_new, ptr_new, r_new, next_s)\n",
    "        outputs = (logits_mem, logits_buf, logits_state)\n",
    "        \n",
    "        return new_carry, outputs\n",
    "\n",
    "class NeuralStackMachine(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, x, true_actions, true_states, use_forcing):\n",
    "        \"\"\"\n",
    "        x: [Batch, Seq]\n",
    "        true_actions: [Batch, Seq]\n",
    "        true_states: [Batch, Seq]\n",
    "        use_forcing: bool (Scalar)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = x.shape\n",
    "        \n",
    "        #initial carry\n",
    "        init_stack = jnp.zeros((batch_size, StackMachineCell.stack_depth), dtype=jnp.int32)\n",
    "        init_ptr = jnp.zeros((batch_size,), dtype=jnp.int32)\n",
    "        init_reg = jnp.zeros((batch_size,), dtype=jnp.int32)\n",
    "        init_state = jnp.zeros((batch_size,), dtype=jnp.int32)\n",
    "        carry = (init_stack, init_ptr, init_reg, init_state)\n",
    "        \n",
    "        forcing_seq = jnp.full((batch_size, seq_len), use_forcing, dtype=jnp.int32)\n",
    "        \n",
    "        scan_inputs = (x, true_actions, true_states, forcing_seq)\n",
    "        \n",
    "        # in_axes=1 we scan over the time dimension\n",
    "        # out_axes=1 we stack along the time dimension\n",
    "        scan_layer = nn.scan(\n",
    "            StackMachineCell,\n",
    "            variable_broadcast=\"params\",\n",
    "            split_rngs={\"params\": False},\n",
    "            in_axes=1, \n",
    "            out_axes=1\n",
    "        )\n",
    "        \n",
    "        final_carry, sequence_outputs = scan_layer()(carry, scan_inputs)\n",
    "        \n",
    "        return sequence_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3fd786",
   "metadata": {},
   "source": [
    "---\n",
    "Model 2: Soft Stack\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19bd978f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_update_stack(stack, ptr_dist, action_probs):\n",
    "    \"\"\"\n",
    "    \"soft\" stack update\n",
    "    \n",
    "    stack: [Depth, Stack_Vocab_Size]\n",
    "    ptr_dist: [Depth] (Distribution)\n",
    "    action_probs: [4] (NOOP, PUSH0, PUSH1, POP)\n",
    "    \"\"\"\n",
    "    # actions\n",
    "    p_noop = action_probs[ACT_NOOP]\n",
    "    p_push0 = action_probs[ACT_PUSH_0]\n",
    "    p_push1 = action_probs[ACT_PUSH_1]\n",
    "    p_pop = action_probs[ACT_POP]\n",
    "    \n",
    "    total_push = p_push0 + p_push1\n",
    "    \n",
    "    #READ\n",
    "    # Shift pointer up\n",
    "    pop_ptr_dist = jnp.roll(ptr_dist, -1)\n",
    "    \n",
    "    #boundary\n",
    "    pop_ptr_dist = pop_ptr_dist.at[-1].set(0.0)\n",
    "    pop_ptr_dist = pop_ptr_dist.at[0].add(ptr_dist[0])\n",
    "\n",
    "    #weighted stack\n",
    "    read_val_vec = jnp.sum(stack * pop_ptr_dist[:, None], axis=0)\n",
    "    \n",
    "    #WRITE\n",
    "    val_push_0 = p_push0 / total_push\n",
    "    val_push_1 = p_push1 / total_push\n",
    "    \n",
    "\n",
    "    write_vec = jnp.array([0.0, 1.0, 0.0]) * val_push_0 + jnp.array([0.0, 0.0, 1.0]) * val_push_1\n",
    "    \n",
    "    # mix old stack and new value\n",
    "    write_gate = ptr_dist[:, None] * total_push\n",
    "    stack_new = stack * (1.0 - write_gate) + write_vec[None, :] * write_gate\n",
    "    \n",
    "    # Erase\n",
    "    pop_gate = pop_ptr_dist[:, None] * p_pop\n",
    "    null_vec = jnp.array([1.0, 0.0, 0.0])\n",
    "    stack_new = stack_new * (1.0 - pop_gate) + null_vec[None, :] * pop_gate\n",
    "    \n",
    "    #move ptr\n",
    "    \n",
    "    push_ptr_dist = jnp.roll(ptr_dist, 1)\n",
    "    push_ptr_dist = push_ptr_dist.at[0].set(0.0) #bottom doesnt wrap to top\n",
    "    \n",
    "    # all movement possibilities\n",
    "    new_ptr_dist = (p_noop * ptr_dist) + \\\n",
    "                   (total_push * push_ptr_dist) + \\\n",
    "                   (p_pop * pop_ptr_dist)\n",
    "                   \n",
    "    # Scale read value by probability of popping\n",
    "    r_t = read_val_vec * p_pop\n",
    "    \n",
    "    return stack_new, new_ptr_dist, r_t\n",
    "\n",
    "\n",
    "class SoftStackMachineCell(nn.Module):\n",
    "    stack_depth: int = STACK_DEPTH\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, carry, inputs):\n",
    "        stack, ptr_dist, r_prev, s_prev = carry\n",
    "        x_t, true_act, true_s, use_forcing = inputs\n",
    "        \n",
    "        #Embeddings\n",
    "        x_emb = nn.Embed(VOCAB_SIZE, HIDDEN_DIM)(x_t)\n",
    "        s_emb = jax.nn.one_hot(s_prev, NUM_STATES)\n",
    "        \n",
    "        # r_prev is a float vector \n",
    "        r_emb = nn.Dense(HIDDEN_DIM)(r_prev) \n",
    "        \n",
    "        flat_input = jnp.concatenate([x_emb, s_emb, r_emb], axis=-1)\n",
    "        \n",
    "        # Controller\n",
    "        logits_mem = nn.Dense(NUM_MEM_ACTIONS, name=\"head_mem\")(flat_input)\n",
    "        logits_buf = nn.Dense(NUM_BUF_ACTIONS, name=\"head_buf\")(flat_input)\n",
    "        logits_state = nn.Dense(NUM_STATES, name=\"head_state\")(flat_input)\n",
    "        \n",
    "        # action selection with softmax\n",
    "        action_probs = nn.softmax(logits_mem)\n",
    "        \n",
    "        # true action if we use forving\n",
    "        true_act_onehot = jax.nn.one_hot(true_act, NUM_MEM_ACTIONS)\n",
    "        \n",
    "        forcing_gate = use_forcing[:, None] \n",
    "        \n",
    "        #can mix true action and predicted action if needed? \n",
    "        # but use_forcing is usually either 1 or 0 for all\n",
    "        probs_to_exec = (forcing_gate * true_act_onehot) + ((1.0 - forcing_gate) * action_probs)\n",
    "        \n",
    "        # state update -still argmax?\n",
    "        pred_state = jnp.argmax(logits_state, axis=-1)\n",
    "        next_s = jnp.where(use_forcing > 0, true_s, pred_state)\n",
    "        \n",
    "        # soft stack update\n",
    "        stack_new, ptr_new, r_new = jax.vmap(soft_update_stack)(stack, ptr_dist, probs_to_exec)\n",
    "        \n",
    "        new_carry = (stack_new, ptr_new, r_new, next_s)\n",
    "        return new_carry, (logits_mem, logits_buf, logits_state)\n",
    "\n",
    "class NeuralSoftStackMachine(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, x, true_actions, true_states, use_forcing):\n",
    "        batch_size, seq_len = x.shape\n",
    "        \n",
    "        init_stack = jnp.zeros((batch_size, STACK_DEPTH, STACK_VOCAB_SIZE))\n",
    "        init_ptr = jnp.zeros((batch_size, STACK_DEPTH))\n",
    "        init_ptr = init_ptr.at[:, 0].set(1.0) #everything at NULL\n",
    "        init_reg = jnp.zeros((batch_size, STACK_VOCAB_SIZE))\n",
    "        init_state = jnp.zeros((batch_size,), dtype=jnp.int32)\n",
    "        \n",
    "        carry = (init_stack, init_ptr, init_reg, init_state)\n",
    "        \n",
    "        forcing_seq = jnp.full((batch_size, seq_len), use_forcing, dtype=jnp.float32)\n",
    "        \n",
    "        scan_inputs = (x, true_actions, true_states, forcing_seq)\n",
    "        \n",
    "        scan_layer = nn.scan(\n",
    "            SoftStackMachineCell,\n",
    "            variable_broadcast=\"params\",\n",
    "            split_rngs={\"params\": False},\n",
    "            in_axes=1, out_axes=1\n",
    "        )\n",
    "        \n",
    "        _, outputs = scan_layer()(carry, scan_inputs)\n",
    "        return outputs\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12511e0f",
   "metadata": {},
   "source": [
    "---\n",
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1039fd8",
   "metadata": {},
   "source": [
    "Using hard Stack RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e17c4b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Step 0 | Loss: 3.4264 | Buffer Acc: 0.00%\n",
      "Step 200 | Loss: 1.1488 | Buffer Acc: 75.50%\n",
      "Step 400 | Loss: 0.7174 | Buffer Acc: 87.14%\n",
      "Step 600 | Loss: 0.4818 | Buffer Acc: 100.00%\n",
      "Step 800 | Loss: 0.3630 | Buffer Acc: 100.00%\n",
      "Step 1000 | Loss: 0.2873 | Buffer Acc: 100.00%\n",
      "Step 1200 | Loss: 0.2301 | Buffer Acc: 100.00%\n",
      "Step 1400 | Loss: 0.1963 | Buffer Acc: 100.00%\n",
      "Step 1600 | Loss: 0.1682 | Buffer Acc: 100.00%\n",
      "Step 1800 | Loss: 0.1476 | Buffer Acc: 100.00%\n",
      "\n",
      "Inference (No Teacher Forcing)...\n",
      "Target: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 1 1 1 1 2 2 2 2 2 2 2 1 1\n",
      " 1 1 2 2 2 1 1 2 1 1 2 2 2 1 1 1 2 1 1 1 2 2 2 2 1 1 2 1 1 2 1 1 1 1 2 2 1\n",
      " 1 2 1 1 2 2 0 0 0 0]\n",
      "Pred:   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 1 1 1 1 2 2 2 2 2 2 2 1 1\n",
      " 1 1 2 2 2 1 1 2 1 1 2 2 2 1 1 1 2 1 1 1 2 2 2 2 1 1 2 1 1 2 1 1 1 1 2 2 1\n",
      " 1 2 1 1 2 2 0 0 0 0]\n",
      "Match:  True\n"
     ]
    }
   ],
   "source": [
    "key = jax.random.PRNGKey(42)\n",
    "\n",
    "model = NeuralStackMachine()\n",
    "\n",
    "# Init with forcing=True\n",
    "dummy_x = jnp.zeros((BATCH_SIZE, SEQ_LENGTH), dtype=jnp.int32)\n",
    "params = model.init(key, dummy_x, dummy_x, dummy_x, use_forcing=True)\n",
    "\n",
    "tx = optax.adam(LEARNING_RATE)\n",
    "state = train_state.TrainState.create(apply_fn=model.apply, params=params, tx=tx)\n",
    "\n",
    "# Loss Function\n",
    "def loss_fn(params, batch):\n",
    "    inputs, tgt_mem, tgt_buf, tgt_state = batch\n",
    "    logits = model.apply(params, inputs, tgt_mem, tgt_state, use_forcing=True)\n",
    "    l_mem, l_buf, l_state = logits\n",
    "    \n",
    "    loss_m = optax.softmax_cross_entropy_with_integer_labels(l_mem, tgt_mem).mean()\n",
    "    loss_b = optax.softmax_cross_entropy_with_integer_labels(l_buf, tgt_buf).mean()\n",
    "    loss_s = optax.softmax_cross_entropy_with_integer_labels(l_state, tgt_state).mean()\n",
    "\n",
    "    acc = (jnp.argmax(l_buf, -1) == tgt_buf).mean()\n",
    "    \n",
    "    return loss_m + loss_b + loss_s, acc\n",
    "\n",
    "train_step = jax.jit(jax.value_and_grad(loss_fn, has_aux=True))\n",
    "\n",
    "print(\"Training...\")\n",
    "for step in range(STEPS):\n",
    "    key, subkey = jax.random.split(key)\n",
    "    batch = generate_rev_trace( BATCH_SIZE)\n",
    "    \n",
    "    (loss, acc), grads = train_step(state.params, batch)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    \n",
    "    if step % 200 == 0:\n",
    "        print(f\"Step {step} | Loss: {loss:.4f} | Buffer Acc: {acc:.2%}\")\n",
    "        \n",
    "\n",
    "#INFERENCE\n",
    "print(\"\\nInference (No Teacher Forcing)...\")\n",
    "test_batch = generate_rev_trace(1)\n",
    "inp, tgt_m, tgt_b, tgt_s = test_batch\n",
    "\n",
    "# Inference Mode: use_forcing = False\n",
    "logits = model.apply(state.params, inp, tgt_m, tgt_s, use_forcing=False) \n",
    "pred_buf = jnp.argmax(logits[1], -1)\n",
    "\n",
    "print(f\"Target: {tgt_b[0]}\")\n",
    "print(f\"Pred:   {pred_buf[0]}\")\n",
    "print(f\"Match:  {jnp.array_equal(tgt_b[0], pred_buf[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95a465a",
   "metadata": {},
   "source": [
    "--- \n",
    "# Visuals + length generalisation test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6c383c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "\n",
    "def visualize_trace(apply_fn, params, seq_len):\n",
    "    print(f\"--- Visualizing Trace for Length {seq_len} ---\")\n",
    "    \n",
    "    # 1. Generate one test sample\n",
    "    key = jax.random.PRNGKey(42)\n",
    "    batch = generate_rev_trace( 1, seq_len)\n",
    "    inputs, tgt_mem, tgt_buf, tgt_state = batch\n",
    "    \n",
    "    # 2. Run Inference\n",
    "    # FIX: Call apply_fn directly. Do not use .apply()\n",
    "    # We pass use_forcing=False (last argument)\n",
    "    logits = apply_fn(params, inputs, tgt_mem, tgt_state, False)\n",
    "    \n",
    "    pred_mem = jnp.argmax(logits[0], -1)[0] # [Seq]\n",
    "    pred_buf = jnp.argmax(logits[1], -1)[0]\n",
    "    \n",
    "    # 3. Reconstruct Stack History\n",
    "    # Since the model scan doesn't return the stack states, we simulate it \n",
    "    # using the *predicted* actions to see what the controller thinks it's doing.\n",
    "    stack_history = []\n",
    "    stack = np.zeros(STACK_DEPTH, dtype=int) # Depth 20\n",
    "    ptr = 0\n",
    "    \n",
    "    # Move to numpy for simulation loop\n",
    "    pred_mem_np = np.array(pred_mem)\n",
    "    \n",
    "    for t in range(seq_len*2+1):\n",
    "        act = pred_mem_np[t]\n",
    "        \n",
    "        # Logic matches update_stack:\n",
    "        # 1 (PUSH 0), 2 (PUSH 1), 3 (POP)\n",
    "        if act == 1: # Push 0\n",
    "            stack[ptr] = 1 # We store '1' for bit 0 to distinguish from empty\n",
    "            ptr += 1\n",
    "        elif act == 2: # Push 1\n",
    "            stack[ptr] = 2 # We store '2' for bit 1\n",
    "            ptr += 1\n",
    "        elif act == 3: # Pop\n",
    "            if ptr > 0:\n",
    "                ptr -= 1\n",
    "                stack[ptr] = 0\n",
    "        \n",
    "        # Store copy of current stack state\n",
    "        stack_history.append(stack.copy())\n",
    "        \n",
    "    stack_history = np.array(stack_history).T # Shape: [Depth, Time]\n",
    "\n",
    "    # 4. Plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    # Color map: White=Empty, Light Grey=0, Dark Grey=1\n",
    "    # We map our stack values (0, 1, 2) to colors\n",
    "    sns.heatmap(stack_history, cmap=\"Greys\", cbar=False, \n",
    "                linewidths=0.5, linecolor='lightgray', ax=ax, vmin=0, vmax=2)\n",
    "    \n",
    "    # Annotate x-axis with Actions\n",
    "    # Map raw integers to labels\n",
    "    actions_map = {0: '.', 1: 'P0', 2: 'P1', 3: 'POP'}\n",
    "    act_labels = [actions_map[a] for a in pred_mem_np]\n",
    "    \n",
    "    ax.set_xticks(np.arange(seq_len*2 + 1) + 0.5)\n",
    "    ax.set_xticklabels(act_labels, rotation=0, fontsize=9)\n",
    "    ax.set_xlabel(\"Time Step (Actions)\")\n",
    "    \n",
    "    \n",
    "    # Map predictions: 0->., 1->0, 2->1\n",
    "    buf_labels = []\n",
    "    for b in np.array(pred_buf):\n",
    "        if b == 0: buf_labels.append('.')\n",
    "        elif b == 1: buf_labels.append('0')\n",
    "        elif b == 2: buf_labels.append('1')\n",
    "        \n",
    "\n",
    "    # Draw a vertical line where the '=' sign was (approximate by looking at inputs)\n",
    "    # The input sequence is in inputs[0]\n",
    "    eq_indices = np.where(inputs[0] == 3)[0] # 3 is '='\n",
    "    if len(eq_indices) > 0:\n",
    "        plt.axvline(x=eq_indices[0] + 0.5, color='red', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    plt.title(f\"Memory Trace (Red Line = Switch State)\")\n",
    "    plt.ylabel(\"Stack Depth\")\n",
    "    plt.yticks([]) # Hide depth numbers for cleanliness\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Usage:\n",
    "# Pass state.apply_fn (the function) and state.params (the weights)\n",
    "visualize_trace(state.apply_fn, state.params, SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d931725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing Generalization on Length 120 ---\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 2 1 2 1 2 2 2 1 1 2 2 2 2 2 2 2 1 1 2 1 2 1 2 1 2 2 1\n",
      " 2 2 2 1 1 1 1 1 2 2 2 2 1 1 2 1 1 1 1 1 2 1 1 1 1 1 2 2 2 2 1 1 1 1 2 2 1\n",
      " 2 1 1 2 2 2 2 1 2 1 2 2 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 2 2 1\n",
      " 2 2 2 1 1 1 1 1 2 2 2 2 1 1 2 1 1 1 1 1 2 1 1 1 1 1 2 2 2 2 1 1 1 1 2 2 1\n",
      " 2 1 1 2 2 2 2 1 2 1 2 2 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Accuracy on Length 120: 97.01%\n"
     ]
    }
   ],
   "source": [
    "def test_generalization(state, test_len):\n",
    "    print(f\"\\n--- Testing Generalization on Length {test_len} ---\")\n",
    "    # Generate batch with new length\n",
    "    \n",
    "    key = jax.random.PRNGKey(101)\n",
    "    batch = generate_rev_trace(1000, test_len) # Batch 100\n",
    "    inputs, tgt_mem, tgt_buf, tgt_state = batch\n",
    "    \n",
    "    # inference\n",
    "    logits = state.apply_fn(state.params, inputs, tgt_mem, tgt_state, False)\n",
    "    pred_buf = jnp.argmax(logits[1], -1)\n",
    "    \n",
    "    # Accuracy\n",
    "    correct = (pred_buf == tgt_buf)\n",
    "    #print an incorrect example?\n",
    "    for i in range(1000) :\n",
    "      if not jnp.array_equal(tgt_buf[i], pred_buf[i]):\n",
    "        print(tgt_buf[i], pred_buf[i])\n",
    "        break\n",
    "    acc = correct.mean()\n",
    "    \n",
    "    print(f\"Accuracy on Length {test_len}: {acc:.2%}\")\n",
    "\n",
    "test_generalization(state, SEQ_LENGTH * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4d5da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "\n",
    "def visualize_failure_zoomed(apply_fn, params, seq_len):\n",
    "    print(f\"Searching for a failure case at length {seq_len}...\")\n",
    "    \n",
    "    max_tries = 1000\n",
    "    found_fail = False\n",
    "    \n",
    "    for i in range(max_tries):\n",
    "        # 1. Generate Batch (No Key Argument)\n",
    "        # Assuming generate_rev_trace returns (inputs, tgt_mem, tgt_buf, tgt_state, mask)\n",
    "        # If your function doesn't return a mask, we can ignore the 5th return value\n",
    "        batch_out = generate_rev_trace(1, seq_len)\n",
    "        inputs, tgt_mem, tgt_buf, tgt_state = batch_out[:4]\n",
    "        \n",
    "        # 2. Run Inference\n",
    "        logits = apply_fn(params, inputs, tgt_mem, tgt_state, False)\n",
    "        pred_buf = jnp.argmax(logits[1], -1)[0]\n",
    "        tgt_buf_sq = tgt_buf[0]\n",
    "        \n",
    "        # 3. Detect \"Active\" Region for Zooming\n",
    "        # Find the last index where the target buffer or input is not 0 (PAD)\n",
    "        # We look for the last 'event' to crop the plot\n",
    "        last_input_idx = np.max(np.where(inputs[0] != 0)[0])\n",
    "        last_tgt_idx = np.max(np.where(tgt_buf_sq != 0)[0]) if np.any(tgt_buf_sq != 0) else 0\n",
    "        \n",
    "        # The relevant end is the later of the two, plus a small margin (e.g. 2 steps)\n",
    "        relevant_len = max(last_input_idx, last_tgt_idx) + 3\n",
    "        relevant_len = min(relevant_len, seq_len) # Clamp to max length\n",
    "        \n",
    "        # 4. Check for Failure (only in the relevant region)\n",
    "        # We compare only up to relevant_len\n",
    "        is_correct = jnp.array_equal(pred_buf[:relevant_len], tgt_buf_sq[:relevant_len])\n",
    "        \n",
    "        if not is_correct:\n",
    "            print(f\"Found failure on attempt {i}! (Relevant Length: {relevant_len})\")\n",
    "            found_fail = True\n",
    "            \n",
    "            # --- PREPARE DATA ---\n",
    "            pred_mem = jnp.argmax(logits[0], -1)[0]\n",
    "            pred_state = jnp.argmax(logits[2], -1)[0]\n",
    "            \n",
    "            # Slice to relevant region\n",
    "            inputs_view = inputs[0, :relevant_len]\n",
    "            pred_mem_view = pred_mem[:relevant_len]\n",
    "            pred_buf_view = pred_buf[:relevant_len]\n",
    "            tgt_buf_view = tgt_buf_sq[:relevant_len]\n",
    "            tgt_state_view = tgt_state[0, :relevant_len]\n",
    "            pred_state_view = pred_state[:relevant_len]\n",
    "            \n",
    "            # --- SIMULATE STACK FOR HEATMAP ---\n",
    "            stack_history = []\n",
    "            stack = np.zeros(100, dtype=int) # Large buffer\n",
    "            ptr = 0\n",
    "            max_ptr_reached = 0\n",
    "            \n",
    "            pred_mem_np = np.array(pred_mem_view)\n",
    "            for t in range(relevant_len):\n",
    "                act = pred_mem_np[t]\n",
    "                if act == 1: stack[ptr] = 1; ptr += 1 # Push 0\n",
    "                elif act == 2: stack[ptr] = 2; ptr += 1 # Push 1\n",
    "                elif act == 3: # Pop\n",
    "                    if ptr > 0: ptr -= 1; stack[ptr] = 0\n",
    "                \n",
    "                max_ptr_reached = max(max_ptr_reached, ptr)\n",
    "                stack_history.append(stack.copy())\n",
    "            \n",
    "            # Transpose for plotting: [Depth, Time]\n",
    "            stack_history = np.array(stack_history).T\n",
    "            \n",
    "            # Crop Stack Depth (Y-Axis Zoom)\n",
    "            # Show up to max depth used + 2 empty rows\n",
    "            display_depth = max(5, max_ptr_reached + 2) \n",
    "            stack_history = stack_history[:display_depth, :]\n",
    "\n",
    "            # --- PLOTTING ---\n",
    "            fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 8), \n",
    "                                           sharex=True, gridspec_kw={'height_ratios': [3, 1]})\n",
    "            \n",
    "            # Plot 1: Stack Heatmap\n",
    "            sns.heatmap(stack_history, cmap=\"Greys\", cbar=False, \n",
    "                        linewidths=0.5, linecolor='lightgray', ax=ax1, vmin=0, vmax=2)\n",
    "            \n",
    "            ax1.set_title(f\"Stack Trace (Zoomed to first {relevant_len} steps)\")\n",
    "            ax1.set_ylabel(\"Stack Depth\")\n",
    "            \n",
    "            # Annotate Top with Buffer Outputs (Pred vs Target)\n",
    "            sec_ax = ax1.secondary_xaxis('top')\n",
    "            sec_ax.set_xticks(np.arange(relevant_len) + 0.5)\n",
    "            \n",
    "            buf_labels = []\n",
    "            for t in range(relevant_len):\n",
    "                p = pred_buf_view[t]\n",
    "                g = tgt_buf_view[t]\n",
    "                \n",
    "                if p == g and p == 0: lbl = \".\"     # Correct Silence\n",
    "                elif p == g: lbl = \"âœ“\"              # Correct Emit\n",
    "                elif g == 0: lbl = f\"E({p-1})\"      # Hallucination (Expected Silence)\n",
    "                else: lbl = \"X\"                     # Wrong Bit\n",
    "                buf_labels.append(lbl)\n",
    "            \n",
    "            sec_ax.set_xticklabels(buf_labels, fontsize=9, fontweight='bold')\n",
    "            \n",
    "            # Plot 2: State Stability\n",
    "            ax2.plot(tgt_state_view, label=\"Target State\", color='green', linestyle='--', linewidth=2)\n",
    "            ax2.plot(pred_state_view, label=\"Predicted State\", color='red', alpha=0.8, linewidth=2)\n",
    "            ax2.set_yticks([0, 1])\n",
    "            ax2.set_yticklabels([\"READ\", \"WRITE\"])\n",
    "            ax2.set_ylabel(\"Controller State\")\n",
    "            ax2.legend(loc='upper right')\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Mark the Equal Sign\n",
    "            eq_indices = np.where(inputs_view == 3)[0]\n",
    "            if len(eq_indices) > 0:\n",
    "                eq_x = eq_indices[0] + 0.5\n",
    "                ax1.axvline(x=eq_x, color='blue', linestyle='--', alpha=0.5)\n",
    "                ax2.axvline(x=eq_x, color='blue', linestyle='--', alpha=0.5)\n",
    "                ax1.text(eq_x, display_depth - 0.5, \"EQ\", color='blue', ha='center')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            break\n",
    "\n",
    "    if not found_fail:\n",
    "        print(\"No failures found in 1000 attempts.\")\n",
    "\n",
    "# Usage\n",
    "visualize_failure_zoomed(state.apply_fn, state.params, 120)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb9bdde",
   "metadata": {},
   "source": [
    "---\n",
    "# Train w/ no teacher forcing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2628ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Step 0 | Loss: 3.4580 | Buffer Acc: 11.57%\n",
      "Step 200 | Loss: 1.6783 | Buffer Acc: 76.03%\n",
      "Step 400 | Loss: 0.8569 | Buffer Acc: 87.41%\n",
      "Step 600 | Loss: 0.4480 | Buffer Acc: 100.00%\n",
      "Step 800 | Loss: 0.3113 | Buffer Acc: 100.00%\n",
      "Step 1000 | Loss: 0.2303 | Buffer Acc: 100.00%\n",
      "Step 1200 | Loss: 0.1871 | Buffer Acc: 100.00%\n",
      "Step 1400 | Loss: 0.1580 | Buffer Acc: 100.00%\n",
      "Step 1600 | Loss: 0.1401 | Buffer Acc: 100.00%\n",
      "Step 1800 | Loss: 0.1269 | Buffer Acc: 100.00%\n",
      "\n",
      "Inference (No Teacher Forcing)...\n",
      "Target: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2 2 1 2 2 1 2 2 2 2 1 2 1\n",
      " 1 2 2 2 1 1 1 1 1 1 1 1 1 1 2 1 2 2 1 1 1 2 1 1 2 2 1 1 2 1 1 2 1 2 1 1 2\n",
      " 2 2 2 2 1 1 0 0 0 0]\n",
      "Pred:   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2 2 1 2 2 1 2 2 2 2 1 2 1\n",
      " 1 2 2 2 1 1 1 1 1 1 1 1 1 1 2 1 2 2 1 1 1 2 1 1 2 2 1 1 2 1 1 2 1 2 1 1 2\n",
      " 2 2 2 2 1 1 0 0 0 0]\n",
      "Match:  True\n"
     ]
    }
   ],
   "source": [
    "key = jax.random.PRNGKey(42)\n",
    "\n",
    "# Initialize\n",
    "model = NeuralStackMachine()\n",
    "\n",
    "\n",
    "# Init with forcing=False\n",
    "dummy_x = jnp.zeros((BATCH_SIZE, SEQ_LENGTH), dtype=jnp.int32)\n",
    "params = model.init(key, dummy_x, dummy_x, dummy_x, use_forcing=False)\n",
    "\n",
    "tx = optax.adam(LEARNING_RATE)\n",
    "state = train_state.TrainState.create(apply_fn=model.apply, params=params, tx=tx)\n",
    "\n",
    "# Loss Function\n",
    "def loss_fn(params, batch):\n",
    "    inputs, tgt_mem, tgt_buf, tgt_state = batch\n",
    "    logits = model.apply(params, inputs, tgt_mem, tgt_state, use_forcing=False)\n",
    "    l_mem, l_buf, l_state = logits\n",
    "    \n",
    "    loss_m = optax.softmax_cross_entropy_with_integer_labels(l_mem, tgt_mem).mean()\n",
    "    loss_b = optax.softmax_cross_entropy_with_integer_labels(l_buf, tgt_buf).mean()\n",
    "    loss_s = optax.softmax_cross_entropy_with_integer_labels(l_state, tgt_state).mean()\n",
    "\n",
    "    acc = (jnp.argmax(l_buf, -1) == tgt_buf).mean()\n",
    "    \n",
    "    return loss_m + loss_b + loss_s, acc\n",
    "\n",
    "train_step = jax.jit(jax.value_and_grad(loss_fn, has_aux=True))\n",
    "\n",
    "print(\"Training...\")\n",
    "for step in range(STEPS):\n",
    "    key, subkey = jax.random.split(key)\n",
    "    batch = generate_rev_trace( BATCH_SIZE)\n",
    "    \n",
    "    (loss, acc), grads = train_step(state.params, batch)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    \n",
    "    if step % 200 == 0:\n",
    "        print(f\"Step {step} | Loss: {loss:.4f} | Buffer Acc: {acc:.2%}\")\n",
    "        \n",
    "\n",
    "#INFERENCE\n",
    "print(\"\\nInference (No Teacher Forcing)...\")\n",
    "test_batch = generate_rev_trace(1)\n",
    "inp, tgt_m, tgt_b, tgt_s = test_batch\n",
    "\n",
    "# Inference Mode: use_forcing = False\n",
    "logits = model.apply(state.params, inp, tgt_m, tgt_s, use_forcing=False) \n",
    "pred_buf = jnp.argmax(logits[1], -1)\n",
    "\n",
    "print(f\"Target: {tgt_b[0]}\")\n",
    "print(f\"Pred:   {pred_buf[0]}\")\n",
    "print(f\"Match:  {jnp.array_equal(tgt_b[0], pred_buf[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3e38d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing Generalization on Length 120 ---\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 1 1 2 1 1 1 1 1 2 2 2 1 1 2 2 1 2 2 1 2 2 1 2 2 2 2 2 1 1 1 2 1 1 1\n",
      " 2 2 1 1 1 1 1 2 2 1 2 1 1 2 2 1 1 2 1 2 1 2 1 2 2 1 2 2 2 2 1 1 1 2 2 2 1\n",
      " 1 2 2 1 2 2 1 2 1 1 1 2 1 1 1 2 1 2 1 2 1 2 2 2 1 2 1 1 2 1 2 1 1 1 1 2 2\n",
      " 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 1 2 1 2 2 1 2 2 2 2 1 1 1 2 2 2 1\n",
      " 1 2 2 1 2 2 1 2 1 1 1 2 1 1 1 2 1 2 1 2 1 2 2 2 1 2 1 1 2 1 2 1 1 1 1 2 2\n",
      " 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Accuracy on Length 120: 96.98%\n"
     ]
    }
   ],
   "source": [
    "test_generalization(state, SEQ_LENGTH * 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f36dd6f",
   "metadata": {},
   "source": [
    "---\n",
    "# One hot for input embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "554fa01f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Step 0 | Loss: 3.2627 | Buffer Acc: 13.87%\n",
      "Step 200 | Loss: 1.7065 | Buffer Acc: 100.00%\n",
      "Step 400 | Loss: 0.9742 | Buffer Acc: 100.00%\n",
      "Step 600 | Loss: 0.6106 | Buffer Acc: 100.00%\n",
      "Step 800 | Loss: 0.3666 | Buffer Acc: 100.00%\n",
      "Step 1000 | Loss: 0.2718 | Buffer Acc: 100.00%\n",
      "Step 1200 | Loss: 0.2028 | Buffer Acc: 100.00%\n",
      "Step 1400 | Loss: 0.1689 | Buffer Acc: 100.00%\n",
      "Step 1600 | Loss: 0.1416 | Buffer Acc: 100.00%\n",
      "Step 1800 | Loss: 0.1209 | Buffer Acc: 100.00%\n",
      "\n",
      "Inference (No Teacher Forcing)...\n",
      "Target: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 1 2 1 2 1 1 2 1 1 2 1 1 2 1 2 2 2 1 2 1 1 2 2 2 2 1 1 2 1\n",
      " 1 2 1 1 1 1 2 1 1 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Pred:   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 1 2 1 2 1 1 2 1 1 2 1 1 2 1 2 2 2 1 2 1 1 2 2 2 2 1 1 2 1\n",
      " 1 2 1 1 1 1 2 1 1 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Match:  True\n"
     ]
    }
   ],
   "source": [
    "HIDDEN_DIM = 4\n",
    "# Initialize\n",
    "model = NeuralStackMachine()\n",
    "\n",
    "\n",
    "# Init with forcing=True\n",
    "dummy_x = jnp.zeros((BATCH_SIZE, SEQ_LENGTH), dtype=jnp.int32)\n",
    "params = model.init(key, dummy_x, dummy_x, dummy_x, use_forcing=True)\n",
    "\n",
    "tx = optax.adam(LEARNING_RATE)\n",
    "state = train_state.TrainState.create(apply_fn=model.apply, params=params, tx=tx)\n",
    "\n",
    "# Loss Function\n",
    "def loss_fn(params, batch):\n",
    "    inputs, tgt_mem, tgt_buf, tgt_state = batch\n",
    "    logits = model.apply(params, inputs, tgt_mem, tgt_state, use_forcing=True)\n",
    "    l_mem, l_buf, l_state = logits\n",
    "    \n",
    "    loss_m = optax.softmax_cross_entropy_with_integer_labels(l_mem, tgt_mem).mean()\n",
    "    loss_b = optax.softmax_cross_entropy_with_integer_labels(l_buf, tgt_buf).mean()\n",
    "    loss_s = optax.softmax_cross_entropy_with_integer_labels(l_state, tgt_state).mean()\n",
    "\n",
    "    acc = (jnp.argmax(l_buf, -1) == tgt_buf).mean()\n",
    "    \n",
    "    return loss_m + loss_b + loss_s, acc\n",
    "\n",
    "train_step = jax.jit(jax.value_and_grad(loss_fn, has_aux=True))\n",
    "\n",
    "print(\"Training...\")\n",
    "for step in range(STEPS):\n",
    "    key, subkey = jax.random.split(key)\n",
    "    batch = generate_rev_trace( BATCH_SIZE)\n",
    "    \n",
    "    (loss, acc), grads = train_step(state.params, batch)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    \n",
    "    if step % 200 == 0:\n",
    "        print(f\"Step {step} | Loss: {loss:.4f} | Buffer Acc: {acc:.2%}\")\n",
    "        \n",
    "\n",
    "#INFERENCE\n",
    "print(\"\\nInference (No Teacher Forcing)...\")\n",
    "test_batch = generate_rev_trace(1)\n",
    "inp, tgt_m, tgt_b, tgt_s = test_batch\n",
    "\n",
    "# Inference Mode: use_forcing = False\n",
    "logits = model.apply(state.params, inp, tgt_m, tgt_s, use_forcing=False) \n",
    "pred_buf = jnp.argmax(logits[1], -1)\n",
    "\n",
    "print(f\"Target: {tgt_b[0]}\")\n",
    "print(f\"Pred:   {pred_buf[0]}\")\n",
    "print(f\"Match:  {jnp.array_equal(tgt_b[0], pred_buf[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eaadee2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing Generalization on Length 240 ---\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 2 1 2 2 2 2 2 2 2 2 2 1 2 2 1 1 2 2 1 1 1 1 2 1 1 1 2\n",
      " 1 1 1 2 1 2 1 2 1 2 2 1 2 1 2 1 1 2 2 2 1 1 1 1 1 2 2 2 2 1 1 1 2 1 2 2 2\n",
      " 2 2 1 2 1 2 1 1 1 2 2 1 2 2 2 2 2 2 2 2 1 2 2 1 1 1 2 1 1 1 2 2 2 1 2 2 2\n",
      " 2 2 2 1 2 2 2 2 2 2 2 2 2 2 1 2 1 1 1 1 1 2 1 2 1 1 2 1 1 1 2 1 1 1 1 1 1\n",
      " 1 1 2 2 2 2 1 2 2 1 1 1 1 1 1 1 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 1 2 1 2 1 1 2 2 2 1 1 1 1 1 2 2 2 2 1 1 1 2 1 2 2 2\n",
      " 2 2 1 2 1 2 1 1 1 2 2 1 2 2 2 2 2 2 2 2 1 2 2 1 1 1 2 1 1 1 2 2 2 1 2 2 2\n",
      " 2 2 2 1 2 2 2 2 2 2 2 2 2 2 1 2 1 1 1 1 1 2 1 2 1 1 2 1 1 1 2 1 1 1 1 1 1\n",
      " 1 1 2 2 2 2 1 2 2 1 1 1 1 1 1 1 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Accuracy on Length 240: 96.73%\n"
     ]
    }
   ],
   "source": [
    "test_generalization(state, SEQ_LENGTH * 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc16d48",
   "metadata": {},
   "source": [
    "- compare with softer stacks\n",
    "- more stacks?\n",
    "- benchmark against chomsky models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e4fd1b",
   "metadata": {},
   "source": [
    "----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8158e2b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Step 0 | Loss: nan | Buffer Acc: 53.82%\n",
      "Step 200 | Loss: nan | Buffer Acc: 74.15%\n",
      "Step 400 | Loss: nan | Buffer Acc: 71.84%\n",
      "Step 600 | Loss: nan | Buffer Acc: 73.39%\n",
      "Step 800 | Loss: nan | Buffer Acc: 73.50%\n",
      "Step 1000 | Loss: nan | Buffer Acc: 75.52%\n",
      "Step 1200 | Loss: nan | Buffer Acc: 76.68%\n",
      "Step 1400 | Loss: nan | Buffer Acc: 72.34%\n",
      "Step 1600 | Loss: nan | Buffer Acc: 70.61%\n",
      "Step 1800 | Loss: nan | Buffer Acc: 76.98%\n",
      "\n",
      "Inference (No Teacher Forcing)...\n",
      "Target: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 1 1 1 1 2\n",
      " 2 2 1 1 2 1 2 2 2 2 1 1 1 1 2 2 1 2 2 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0]\n",
      "Pred:   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0]\n",
      "Match:  False\n"
     ]
    }
   ],
   "source": [
    "key = jax.random.PRNGKey(42)\n",
    "\n",
    "# Initialize\n",
    "model = NeuralSoftStackMachine()\n",
    "\n",
    "\n",
    "# Init with forcing=True\n",
    "dummy_x = jnp.zeros((BATCH_SIZE, SEQ_LENGTH), dtype=jnp.int32)\n",
    "params = model.init(key, dummy_x, dummy_x, dummy_x, use_forcing=True)\n",
    "\n",
    "tx = optax.adam(LEARNING_RATE)\n",
    "state = train_state.TrainState.create(apply_fn=model.apply, params=params, tx=tx)\n",
    "\n",
    "# Loss Function\n",
    "def loss_fn(params, batch):\n",
    "    inputs, tgt_mem, tgt_buf, tgt_state = batch\n",
    "    logits = model.apply(params, inputs, tgt_mem, tgt_state, use_forcing=True)\n",
    "    l_mem, l_buf, l_state = logits\n",
    "    \n",
    "    loss_m = optax.softmax_cross_entropy_with_integer_labels(l_mem, tgt_mem).mean()\n",
    "    loss_b = optax.softmax_cross_entropy_with_integer_labels(l_buf, tgt_buf).mean()\n",
    "    loss_s = optax.softmax_cross_entropy_with_integer_labels(l_state, tgt_state).mean()\n",
    "\n",
    "    acc = (jnp.argmax(l_buf, -1) == tgt_buf).mean()\n",
    "    \n",
    "    return loss_m + loss_b + loss_s, acc\n",
    "\n",
    "train_step = jax.jit(jax.value_and_grad(loss_fn, has_aux=True))\n",
    "\n",
    "print(\"Training...\")\n",
    "for step in range(STEPS):\n",
    "    key, subkey = jax.random.split(key)\n",
    "    batch = generate_rev_trace( BATCH_SIZE)\n",
    "    \n",
    "    (loss, acc), grads = train_step(state.params, batch)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    \n",
    "    if step % 200 == 0:\n",
    "        print(f\"Step {step} | Loss: {loss:.4f} | Buffer Acc: {acc:.2%}\")\n",
    "        \n",
    "\n",
    "#INFERENCE\n",
    "print(\"\\nInference (No Teacher Forcing)...\")\n",
    "test_batch = generate_rev_trace(1)\n",
    "inp, tgt_m, tgt_b, tgt_s = test_batch\n",
    "\n",
    "# Inference Mode: use_forcing = False\n",
    "logits = model.apply(state.params, inp, tgt_m, tgt_s, use_forcing=False) \n",
    "pred_buf = jnp.argmax(logits[1], -1)\n",
    "\n",
    "print(f\"Target: {tgt_b[0]}\")\n",
    "print(f\"Pred:   {pred_buf[0]}\")\n",
    "print(f\"Match:  {jnp.array_equal(tgt_b[0], pred_buf[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ab68a589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing Generalization on Length 120 ---\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2 1\n",
      " 2 1 1 2 2 2 2 1 2 1 2 1 1 2 1 2 1 1 1 2 1 1 2 1 1 1 2 1 2 1 1 1 1 1 2 1 2\n",
      " 2 1 1 2 1 1 2 2 2 2 1 1 2 2 1 1 2 2 1 1 1 1 1 2 2 2 1 1 1 2 1 1 2 1 2 1 2\n",
      " 1 1 2 2 2 1 2 2 1 2 2 2 1 2 1 1 1 1 1 2 1 1 2 1 1 1 2 2 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Accuracy on Length 120: 74.66%\n"
     ]
    }
   ],
   "source": [
    "test_generalization(state, SEQ_LENGTH * 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ac17a3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f62f6d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaRNN(nn.Module):\n",
    "    hidden_dim: int = HIDDEN_DIM  \n",
    "    @nn.compact\n",
    "    def __call__(self, x, true_act, true_s, use_forcing):\n",
    "        batch_size, seq_len = x.shape\n",
    "        \n",
    "        lstm = nn.LSTMCell(features=HIDDEN_DIM)\n",
    "\n",
    "        carry = (jnp.zeros((batch_size, HIDDEN_DIM)), \n",
    "                 jnp.zeros((batch_size, HIDDEN_DIM)))\n",
    "        \n",
    "        def cell(carry, x_t):\n",
    "            new_carry, new_h = lstm(carry, nn.Embed(VOCAB_SIZE, HIDDEN_DIM)(x_t))\n",
    "            return new_carry, new_h\n",
    "\n",
    "        _, hidden = nn.scan(cell, variable_broadcast=\"params\", split_rngs={\"params\": False}, in_axes=1, out_axes=1)(carry, x)\n",
    "        \n",
    "        return nn.Dense(NUM_BUF_ACTIONS)(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9611fb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_vanilla():\n",
    "    key = jax.random.PRNGKey(42)\n",
    "    \n",
    "    model = VanillaRNN(hidden_dim=HIDDEN_DIM)\n",
    "    \n",
    "    dummy_x = jnp.zeros((BATCH_SIZE, SEQ_LENGTH), dtype=jnp.int32)\n",
    "    params = model.init(key, dummy_x, dummy_x, dummy_x, False)\n",
    "    \n",
    "    tx = optax.adam(LEARNING_RATE)\n",
    "    state = train_state.TrainState.create(apply_fn=model.apply, params=params, tx=tx)\n",
    "    \n",
    "    # loss on buffer only\n",
    "    @jax.jit\n",
    "    def train_step(state, batch):\n",
    "        inputs, tgt_mem, tgt_buf, tgt_state = batch\n",
    "        \n",
    "        def loss_fn(params):\n",
    "            logits_buf = model.apply(params, inputs, tgt_mem, tgt_state, False)\n",
    "            \n",
    "            loss_b = optax.softmax_cross_entropy_with_integer_labels(logits_buf, tgt_buf).mean()\n",
    "            \n",
    "            acc = (jnp.argmax(logits_buf, -1) == tgt_buf).mean()\n",
    "            return loss_b, acc\n",
    "            \n",
    "        (loss, acc), grads = jax.value_and_grad(loss_fn, has_aux=True)(state.params)\n",
    "        state = state.apply_gradients(grads=grads)\n",
    "        return state, loss, acc\n",
    "\n",
    "    #Train\n",
    "    print(\"Training Vanilla LSTM...\")\n",
    "    for step in range(STEPS):\n",
    "        \n",
    "        batch = generate_rev_trace(BATCH_SIZE)\n",
    "        \n",
    "        state, loss, acc = train_step(state, batch)\n",
    "        \n",
    "        if step % 200 == 0:\n",
    "            print(f\"Step {step} | Loss: {loss:.4f} | Buffer Acc: {acc:.2%}\")\n",
    "            \n",
    "    return state\n",
    "\n",
    "vanilla_state = train_vanilla()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
