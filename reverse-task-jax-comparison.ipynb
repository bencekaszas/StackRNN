{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "073d43cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "import optax\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "SEQ_LENGTH = 60       \n",
    "HIDDEN_DIM = 64\n",
    "STACK_DEPTH = SEQ_LENGTH\n",
    "LEARNING_RATE = 1e-3\n",
    "STEPS = 2000\n",
    "\n",
    "# input\n",
    "VOCAB_PAD, VOCAB_0, VOCAB_1, VOCAB_EQ = 0, 1, 2, 3\n",
    "VOCAB_SIZE = 4\n",
    "\n",
    "# stack\n",
    "STACK_NULL, STACK_0, STACK_1 = 0, 1, 2\n",
    "STACK_VOCAB_SIZE = 3\n",
    "\n",
    "# memory actions\n",
    "ACT_NOOP, ACT_PUSH_0, ACT_PUSH_1, ACT_POP = 0, 1, 2, 3\n",
    "NUM_MEM_ACTIONS = 4\n",
    "\n",
    "# buffer\n",
    "OUT_NOOP, OUT_EMIT_0, OUT_EMIT_1 = 0, 1, 2\n",
    "NUM_BUF_ACTIONS = 3\n",
    "\n",
    "#controller states\n",
    "STATE_READ = 0\n",
    "STATE_WRITE = 1\n",
    "NUM_STATES = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d414b4a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAX Backend: gpu\n",
      "Devices: [CudaDevice(id=0)]\n"
     ]
    }
   ],
   "source": [
    "print(\"JAX Backend:\", jax.default_backend())\n",
    "print(\"Devices:\", jax.devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a7eba0",
   "metadata": {},
   "source": [
    "---\n",
    "## data gen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "898d3fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rev_trace(key, batch_size, seq_length=SEQ_LENGTH):\n",
    "    # seeding numpy RNG from JAX RNG\n",
    "    seed = int(jax.random.randint(key, (), 0, 2**30))\n",
    "    rng = np.random.default_rng(seed)\n",
    "    \n",
    "    lengths = rng.integers(1, seq_length + 1, size=batch_size)\n",
    "    seq_len = 2 * seq_length + 1\n",
    "    \n",
    "    inputs = np.full((batch_size, seq_len), VOCAB_PAD, dtype=np.int32)\n",
    "    tgt_act = np.full((batch_size, seq_len), ACT_NOOP, dtype=np.int32)\n",
    "    tgt_buf = np.full((batch_size, seq_len), OUT_NOOP, dtype=np.int32)\n",
    "    tgt_state = np.full((batch_size, seq_len), STATE_READ, dtype=np.int32)\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        L = lengths[i]\n",
    "        bits = rng.integers(1, 3, size=L) \n",
    "        \n",
    "        #READ\n",
    "        inputs[i, :L] = bits\n",
    "        inputs[i, L] = VOCAB_EQ\n",
    "        tgt_act[i, :L] = bits \n",
    "        tgt_state[i, :L] = STATE_READ\n",
    "        tgt_state[i, L] = STATE_WRITE # Switch at =\n",
    "        \n",
    "        #WRITE\n",
    "        pop_start, pop_end = L, L + L\n",
    "        tgt_act[i, pop_start:pop_end] = ACT_POP\n",
    "        tgt_state[i, pop_start:pop_end] = STATE_WRITE\n",
    "        \n",
    "        \n",
    "        reversed_bits = bits[::-1]\n",
    "        tgt_buf[i, pop_start+1 : pop_end+1] = reversed_bits\n",
    "        \n",
    "            \n",
    "    return jnp.array(inputs), jnp.array(tgt_act), jnp.array(tgt_buf), jnp.array(tgt_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91de8c8",
   "metadata": {},
   "source": [
    "---\n",
    "## Models\n",
    "\n",
    "model 1: Hard stack RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5a689c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hard_update_stack(stack, ptr, action):\n",
    "    # READ\n",
    "    pop_ptr = jnp.maximum(0, ptr - 1)\n",
    "    popped_val = stack[pop_ptr]\n",
    "    \n",
    "    # WRITE\n",
    "    push_val = action \n",
    "    is_push = (action == ACT_PUSH_0) | (action == ACT_PUSH_1)\n",
    "    is_pop = (action == ACT_POP)\n",
    "    \n",
    "    # UPDATE\n",
    "    new_stack_push = stack.at[ptr].set(push_val)\n",
    "    new_ptr_push = ptr + 1\n",
    "    new_stack_pop = stack.at[pop_ptr].set(STACK_NULL)\n",
    "    new_ptr_pop = pop_ptr\n",
    "    \n",
    "    stack = jnp.where(is_push, new_stack_push, stack)\n",
    "    stack = jnp.where(is_pop, new_stack_pop, stack)\n",
    "    ptr = jnp.where(is_push, new_ptr_push, ptr)\n",
    "    ptr = jnp.where(is_pop, new_ptr_pop, ptr)\n",
    "    \n",
    "    r_t = jnp.where(is_pop, popped_val, STACK_NULL)\n",
    "    return stack, ptr, r_t\n",
    "\n",
    "\n",
    "\n",
    "class HardStackMachine(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, x, true_act, true_s, use_forcing):\n",
    "        batch_size, seq_len = x.shape\n",
    "        \n",
    "        # Init Hard State\n",
    "        carry = (\n",
    "            jnp.zeros((batch_size, STACK_DEPTH), dtype=jnp.int32), # Stack\n",
    "            jnp.zeros((batch_size,), dtype=jnp.int32),             # Ptr\n",
    "            jnp.zeros((batch_size,), dtype=jnp.int32),             # Reg\n",
    "            jnp.zeros((batch_size,), dtype=jnp.int32)              # State\n",
    "        )\n",
    "        \n",
    "        def cell(carry, inputs):\n",
    "            stack, ptr, r_prev, s_prev = carry\n",
    "            x_t, t_act, t_s, forcing = inputs\n",
    "            \n",
    "            # Embed\n",
    "            flat = jnp.concatenate([\n",
    "                nn.Embed(VOCAB_SIZE, HIDDEN_DIM)(x_t),\n",
    "                jax.nn.one_hot(s_prev, NUM_STATES),\n",
    "                jax.nn.one_hot(r_prev, STACK_VOCAB_SIZE)\n",
    "            ], axis=-1)\n",
    "            \n",
    "            # Heads\n",
    "            l_mem = nn.Dense(NUM_MEM_ACTIONS)(flat)\n",
    "            l_buf = nn.Dense(NUM_BUF_ACTIONS)(flat)\n",
    "            l_state = nn.Dense(NUM_STATES)(flat)\n",
    "            \n",
    "            pred_act = jnp.argmax(l_mem, axis=-1)\n",
    "            pred_state = jnp.argmax(l_state, axis=-1)\n",
    "            \n",
    "            # Forcing\n",
    "            action_to_exec = jnp.where(forcing > 0, t_act, pred_act)\n",
    "            next_s = jnp.where(forcing > 0, t_s, pred_state)\n",
    "            \n",
    "            # Update\n",
    "            stack, ptr, r_new = jax.vmap(hard_update_stack)(stack, ptr, action_to_exec)\n",
    "            return (stack, ptr, r_new, next_s), (l_mem, l_buf, l_state)\n",
    "        \n",
    "        \n",
    "        scan_in = (x, true_act, true_s, jnp.full((batch_size, seq_len), use_forcing))\n",
    "        scan_layer = nn.scan(\n",
    "            cell, \n",
    "            variable_broadcast=\"params\",\n",
    "            split_rngs={\"params\": False},\n",
    "            in_axes=1, \n",
    "            out_axes=1)\n",
    "        \n",
    "        final_carry, out = scan_layer(carry, scan_in)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcc5f65",
   "metadata": {},
   "source": [
    "---\n",
    "model 2: soft stack RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4654e6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_update_stack(stack, ptr_dist, action_probs):\n",
    "    \n",
    "    p_noop, p_p0, p_p1, p_pop = action_probs[0], action_probs[1], action_probs[2], action_probs[3]\n",
    "    total_push = p_p0 + p_p1\n",
    "    \n",
    "    # 1. READ\n",
    "    pop_ptr_dist = jnp.roll(ptr_dist, -1).at[-1].set(0.0)\n",
    "    pop_ptr_dist = pop_ptr_dist.at[0].add(ptr_dist[0]) # Clamp 0\n",
    "    read_vec = jnp.sum(stack * pop_ptr_dist[:, None], axis=0)\n",
    "    \n",
    "    # 2. WRITE\n",
    "    eps = 1e-9\n",
    "    val_p0, val_p1 = p_p0/(total_push+eps), p_p1/(total_push+eps)\n",
    "    write_vec = jnp.array([0., 1., 0.]) * val_p0 + jnp.array([0., 0., 1.]) * val_p1\n",
    "    \n",
    "    # Gates\n",
    "    write_gate = ptr_dist[:, None] * total_push\n",
    "    pop_gate = pop_ptr_dist[:, None] * p_pop\n",
    "    \n",
    "    # Update Stack\n",
    "    stack = stack * (1. - write_gate) + write_vec[None, :] * write_gate\n",
    "    stack = stack * (1. - pop_gate) + jnp.array([1., 0., 0.])[None, :] * pop_gate\n",
    "    \n",
    "    # 3. MOVE POINTER\n",
    "    push_ptr_dist = jnp.roll(ptr_dist, 1).at[0].set(0.0)\n",
    "    new_ptr_dist = (p_noop * ptr_dist) + (total_push * push_ptr_dist) + (p_pop * pop_ptr_dist)\n",
    "    \n",
    "    r_t = read_vec * p_pop\n",
    "    return stack, new_ptr_dist, r_t\n",
    "\n",
    "\n",
    "class SoftStackMachine(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, x, true_act, true_s, use_forcing):\n",
    "        batch_size, seq_len = x.shape\n",
    "        \n",
    "        # Init Soft State\n",
    "        carry = (\n",
    "            jnp.zeros((batch_size, STACK_DEPTH, STACK_VOCAB_SIZE)), # Stack\n",
    "            jnp.zeros((batch_size, STACK_DEPTH)).at[:,0].set(1.0),  # Ptr Dist\n",
    "            jnp.zeros((batch_size, STACK_VOCAB_SIZE)),              # Reg Vec\n",
    "            jnp.zeros((batch_size,), dtype=jnp.int32)               # State (Discrete)\n",
    "        )\n",
    "        \n",
    "        def cell(carry, inputs):\n",
    "            stack, ptr, r_prev, s_prev = carry\n",
    "            x_t, t_act, t_s, forcing = inputs\n",
    "            \n",
    "            # Embed (Note: r_prev is vector now)\n",
    "            flat = jnp.concatenate([\n",
    "                nn.Embed(VOCAB_SIZE, HIDDEN_DIM)(x_t),\n",
    "                jax.nn.one_hot(s_prev, NUM_STATES),\n",
    "                nn.Dense(HIDDEN_DIM)(r_prev)\n",
    "            ], axis=-1)\n",
    "            \n",
    "            l_mem = nn.Dense(NUM_MEM_ACTIONS)(flat)\n",
    "            l_buf = nn.Dense(NUM_BUF_ACTIONS)(flat)\n",
    "            l_state = nn.Dense(NUM_STATES)(flat)\n",
    "            \n",
    "            # Soft Action Mixing\n",
    "            probs = nn.softmax(l_mem)\n",
    "            t_onehot = jax.nn.one_hot(t_act, NUM_MEM_ACTIONS)\n",
    "            # Interpolate between True One-Hot and Predicted Soft Probs\n",
    "            # forcing shape needs strictly (Batch, 1) here\n",
    "            f_gate = forcing[:, None]\n",
    "            mixed_act = (f_gate * t_onehot) + ((1.0 - f_gate) * probs)\n",
    "            \n",
    "            nxt_s = jnp.where(forcing > 0, t_s, jnp.argmax(l_state, -1))\n",
    "            \n",
    "            # Soft Update\n",
    "            stack, ptr, r_new = jax.vmap(soft_update_stack)(stack, ptr, mixed_act)\n",
    "            return (stack, ptr, r_new, nxt_s), (l_mem, l_buf, l_state)\n",
    "\n",
    "        # forcing needs to be floats for interpolation\n",
    "        scan_in = (x, true_act, true_s, jnp.full((batch_size, seq_len), use_forcing, dtype=jnp.float32))\n",
    "        _, out = nn.scan(cell, variable_broadcast=\"params\", split_rngs={\"params\": False}, in_axes=1, out_axes=1)(carry, scan_in)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bd0d74",
   "metadata": {},
   "source": [
    "---\n",
    "model 3: vanilla RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9269f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaRNN(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, x, true_act, true_s, use_forcing):\n",
    "        # Ignores stack inputs\n",
    "        batch_size, seq_len = x.shape\n",
    "        \n",
    "        lstm = nn.LSTMCell()\n",
    "        carry = (jnp.zeros((batch_size, HIDDEN_DIM)), jnp.zeros((batch_size, HIDDEN_DIM)))\n",
    "        \n",
    "        def cell(carry, x_t):\n",
    "            new_c, new_h = lstm(carry, nn.Embed(VOCAB_SIZE, HIDDEN_DIM)(x_t))\n",
    "            return (new_c, new_h), new_h\n",
    "\n",
    "        _, hidden = nn.scan(cell, variable_broadcast=\"params\", split_rngs={\"params\": False}, in_axes=1, out_axes=1)(carry, x)\n",
    "        return nn.Dense(NUM_BUF_ACTIONS)(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963efe44",
   "metadata": {},
   "source": [
    "---\n",
    "model 4: reinforcement learning style?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de4287cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReinforceMachine(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, x, key):\n",
    "        batch_size, seq_len = x.shape\n",
    "        # Init Hard State + Keys\n",
    "        carry = (\n",
    "            jnp.zeros((batch_size, STACK_DEPTH), dtype=jnp.int32),\n",
    "            jnp.zeros((batch_size,), dtype=jnp.int32),\n",
    "            jnp.zeros((batch_size,), dtype=jnp.int32),\n",
    "            jnp.zeros((batch_size,), dtype=jnp.int32),\n",
    "            jax.random.split(key, batch_size) # Keys for sampling\n",
    "        )\n",
    "        \n",
    "        def cell(carry, x_t):\n",
    "            stack, ptr, r_prev, s_prev, rng = carry\n",
    "            \n",
    "            flat = jnp.concatenate([\n",
    "                nn.Embed(VOCAB_SIZE, HIDDEN_DIM)(x_t),\n",
    "                jax.nn.one_hot(s_prev, NUM_STATES),\n",
    "                jax.nn.one_hot(r_prev, STACK_VOCAB_SIZE)\n",
    "            ], axis=-1)\n",
    "            \n",
    "            l_mem, l_buf, l_state = nn.Dense(NUM_MEM_ACTIONS)(flat), nn.Dense(NUM_BUF_ACTIONS)(flat), nn.Dense(NUM_STATES)(flat)\n",
    "            \n",
    "            # SAMPLING\n",
    "            rng, k1, k2 = jax.random.split(rng, 3)\n",
    "            samp_act = jax.random.categorical(k1, l_mem)\n",
    "            samp_s = jax.random.categorical(k2, l_state)\n",
    "            \n",
    "            # Log Probs\n",
    "            lp_mem = jax.nn.log_softmax(l_mem)[samp_act]\n",
    "            lp_s = jax.nn.log_softmax(l_state)[samp_s]\n",
    "            \n",
    "            # Update (Self-Driven)\n",
    "            stack, ptr, r_new = jax.vmap(hard_update_stack)(stack, ptr, samp_act)\n",
    "            return (stack, ptr, r_new, samp_s, rng), (l_buf, lp_mem, lp_s)\n",
    "\n",
    "        _, out = nn.scan(cell, variable_broadcast=\"params\", split_rngs={\"params\": False}, in_axes=1, out_axes=1)(carry, x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0982ddc",
   "metadata": {},
   "source": [
    "---\n",
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a66ab30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Hard Stack ---\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute '_state'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1300880240.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m \u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipython-input-1300880240.py\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n--- Training {name} ---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_train_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-1300880240.py\u001b[0m in \u001b[0;36mcreate_train_state\u001b[0;34m(model, key)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mdummy_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSEQ_LENGTH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# Init with dummies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdummy_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdummy_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdummy_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_forcing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mtx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLEARNING_RATE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrain_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrainState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapply_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "    \u001b[0;31m[... skipping hidden 9 frame]\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-6513494.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, true_act, true_s, use_forcing)\u001b[0m\n\u001b[1;32m     74\u001b[0m             out_axes=1)\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mfinal_carry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscan_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcarry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscan_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/flax/linen/transforms.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    399\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprewrapped_fns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m     \u001b[0;31m# make a scope-function to transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute '_state'"
     ]
    }
   ],
   "source": [
    "def create_train_state(model, key):\n",
    "    dummy_x = jnp.zeros((BATCH_SIZE, SEQ_LENGTH), dtype=jnp.int32)\n",
    "    # Init with dummies\n",
    "    params = model.init(key, dummy_x, dummy_x, dummy_x, use_forcing=True)\n",
    "    tx = optax.adam(LEARNING_RATE)\n",
    "    return train_state.TrainState.create(apply_fn=model.apply, params=params, tx=tx)\n",
    "\n",
    "@jax.jit\n",
    "def train_step(state, batch):\n",
    "    inputs, tgt_mem, tgt_buf, tgt_state = batch\n",
    "    \n",
    "    def loss_fn(params):\n",
    "        # Run model (Teaching Forcing = True)\n",
    "        out = state.apply_fn(params, inputs, tgt_mem, tgt_state, True)\n",
    "        \n",
    "        # Unpack outputs based on model type\n",
    "        if isinstance(out, tuple): \n",
    "            l_mem, l_buf, l_state = out\n",
    "            loss_m = optax.softmax_cross_entropy_with_integer_labels(l_mem, tgt_mem).mean()\n",
    "            loss_s = optax.softmax_cross_entropy_with_integer_labels(l_state, tgt_state).mean()\n",
    "        else: \n",
    "            l_buf = out\n",
    "            loss_m, loss_s = 0., 0.\n",
    "            \n",
    "        loss_b = optax.softmax_cross_entropy_with_integer_labels(l_buf, tgt_buf).mean()\n",
    "        \n",
    "        # Metrics\n",
    "        acc = jnp.mean(jnp.argmax(l_buf, -1) == tgt_buf)\n",
    "        total_loss = loss_b + loss_m + loss_s\n",
    "        return total_loss, acc\n",
    "\n",
    "    (loss, acc), grads = jax.value_and_grad(loss_fn, has_aux=True)(state.params)\n",
    "    return state.apply_gradients(grads=grads), loss, acc\n",
    "\n",
    "def run_experiment():\n",
    "    key = jax.random.PRNGKey(42)\n",
    "    models = {\n",
    "        \"Hard Stack\": HardStackMachine(),\n",
    "        \"Soft Stack\": SoftStackMachine(),\n",
    "        \"Vanilla RNN\": VanillaRNN()\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"\\n--- Training {name} ---\")\n",
    "        key, init_k = jax.random.split(key)\n",
    "        state = create_train_state(model, init_k)\n",
    "        \n",
    "        history = []\n",
    "        for i in range(STEPS):\n",
    "            key, batch_k = jax.random.split(key)\n",
    "            batch = generate_rev_trace(batch_k, BATCH_SIZE)\n",
    "            \n",
    "            state, loss, acc = train_step(state, batch)\n",
    "            \n",
    "            if i % 100 == 0:\n",
    "                print(f\"Step {i:04d} | Acc: {acc:.2%}\")\n",
    "            history.append(acc)\n",
    "        results[name] = history\n",
    "        \n",
    "    # Comparison Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for name, hist in results.items():\n",
    "        plt.plot(hist, label=name)\n",
    "    plt.xlabel(\"Steps\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Stack vs Vanilla Performance\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff9cc808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Hard Stack ---\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute '_state'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-2695275567.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_comparison\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipython-input-645898911.py\u001b[0m in \u001b[0;36mrun_comparison\u001b[0;34m()\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n--- Training {name} ---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_train_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0mbaseline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-645898911.py\u001b[0m in \u001b[0;36mcreate_train_state\u001b[0;34m(model, key, learning_rate)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;31m# Supervised models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdummy_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdummy_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdummy_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mtx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "    \u001b[0;31m[... skipping hidden 9 frame]\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-6513494.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, true_act, true_s, use_forcing)\u001b[0m\n\u001b[1;32m     74\u001b[0m             out_axes=1)\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mfinal_carry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscan_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcarry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscan_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/flax/linen/transforms.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    399\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprewrapped_fns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m     \u001b[0;31m# make a scope-function to transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute '_state'"
     ]
    }
   ],
   "source": [
    "run_comparison()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
